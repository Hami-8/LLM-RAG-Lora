from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from peft import PeftModel

# æ¨¡å‹ä¸LoRAè·¯å¾„
mode_path = '/root/autodl-tmp/LLM-Research/Meta-Llama-3___1-8B-Instruct'
lora_path = './output/llama3_1_instruct_lora/checkpoint-10'

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    mode_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).eval()
model = PeftModel.from_pretrained(model, model_id=lora_path)

# åˆå§‹ system messageï¼ˆä½ å¯ä»¥æ ¹æ®è§’è‰²æ›¿æ¢ï¼‰
system_prompt = "å‡è®¾ä½ æ˜¯çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›ã€‚"

# å¼€å¯äº¤äº’å¼å¯¹è¯å¾ªç¯
print("ğŸ”® å·²åŠ è½½æ¨¡å‹ï¼Œç°åœ¨ä½ å¯ä»¥å¼€å§‹æé—®ã€‚è¾“å…¥ 'exit' é€€å‡ºã€‚\n")

while True:
    user_input = input("ğŸ‘¤ ä½ ï¼š")
    if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
        print("ğŸ‘‹ å†è§ï¼")
        break

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    # æ„é€  Prompt
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([input_text], return_tensors="pt").to('cuda')

    # æ¨¡å‹ç”Ÿæˆå›å¤
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.95
        )

    # è§£ç å¹¶è¾“å‡ºå›ç­”
    generated_ids = outputs[:, inputs.input_ids.shape[1]:]  # å»é™¤ prompt éƒ¨åˆ†
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print("å¬›å¬›ï¼š", response)
