from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŸå§‹æ¨¡å‹è·¯å¾„
model_path = '/root/autodl-tmp/LLM-Research/Meta-Llama-3___1-8B-Instruct'

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).eval()

# äº¤äº’å¼é—®ç­”
print("ğŸ¤– æ¨¡å‹å·²åŠ è½½ï¼Œç°åœ¨å¯ä»¥æé—®äº†ã€‚è¾“å…¥ 'exit' é€€å‡ºã€‚")

system_prompt = "You are a helpful assistant answering questions"

while True:
    user_input = input("ğŸ‘¤ ä½ ï¼š")
    if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
        print("ğŸ‘‹ å†è§ï¼")
        break

    # æ„é€  promptï¼ˆLLaMA3 æ ¼å¼ï¼‰
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([input_text], return_tensors="pt").to('cuda')

    # æ¨¡å‹ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )

    # è§£ç è¾“å‡º
    generated_ids = outputs[:, inputs.input_ids.shape[1]:]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print("æ¨¡å‹ï¼š", response)
